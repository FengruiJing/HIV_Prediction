{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FengruiJing/HIV_Prediction/blob/main/SpatialRegressionOpenAccess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#U.S. data"
      ],
      "metadata": {
        "id": "NzP3i7KmM1Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SEM"
      ],
      "metadata": {
        "id": "PbudyVdKP4Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import math\n",
        "\n",
        "import libpysal\n",
        "from libpysal.weights import util, W, Queen\n",
        "from scipy.optimize import minimize\n",
        "from numpy.linalg import slogdet\n",
        "from scipy.stats import norm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# For parallel processing if needed\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "###############################################################################\n",
        "# A. Read shapefile and required fields, ensuring unique FIPS codes\n",
        "###############################################################################\n",
        "shp_path = \"US_HIV_Merged_total.shp\"  # Change to your path\n",
        "gdf = gpd.read_file(shp_path)\n",
        "\n",
        "if 'FIPS' not in gdf.columns:\n",
        "    raise KeyError(\"Shapefile is missing the 'FIPS' column\")\n",
        "gdf['FIPS'] = gdf['FIPS'].astype(str).str.lstrip('0')\n",
        "gdf = gdf.drop_duplicates(subset=['FIPS'])\n",
        "\n",
        "# Extract required fields (adjust as necessary)\n",
        "hiv_df = gdf[['FIPS', 'GonorrheaR', 'ChlamydiaR', 'PercenHIVp',\n",
        "              'Population', 'UrbanRural', 'Female', 'Old', 'Black', 'Noinsuranc',\n",
        "              'Poverty', 'crime16to1', 'Dissimilar', 'X', 'Y', 'geometry']].copy()\n",
        "print(\"Loaded data (first 5 rows):\")\n",
        "print(hiv_df.head())\n",
        "\n",
        "###############################################################################\n",
        "# B. Construct four spatial weight matrices helper functions\n",
        "###############################################################################\n",
        "def subset_W(w, subset_ids):\n",
        "    \"\"\"\n",
        "    From the given W object, retain only rows and columns whose IDs are in subset_ids.\n",
        "    Returns a new W object.\n",
        "    \"\"\"\n",
        "    from libpysal.weights import W\n",
        "    subset_ids = set(subset_ids)\n",
        "    new_neighbors = {}\n",
        "    new_weights = {}\n",
        "    for i in w.id_order:\n",
        "        if i not in subset_ids:\n",
        "            continue\n",
        "        old_neighbors = w.neighbors[i]\n",
        "        old_weights = w.weights[i]\n",
        "        filtered_neighbors = []\n",
        "        filtered_weights = []\n",
        "        for nb, wt in zip(old_neighbors, old_weights):\n",
        "            if nb in subset_ids:\n",
        "                filtered_neighbors.append(nb)\n",
        "                filtered_weights.append(wt)\n",
        "        new_neighbors[i] = filtered_neighbors\n",
        "        new_weights[i] = filtered_weights\n",
        "    return W(new_neighbors, new_weights, ids=list(subset_ids))\n",
        "\n",
        "def build_distance_matrix(coords, max_dist_km=200):\n",
        "    \"\"\"\n",
        "    Build an inverse-distance matrix with cutoff. The matrix is row-normalized.\n",
        "    max_dist_km: Distances beyond this are set to 0.\n",
        "    \"\"\"\n",
        "    distance = np.linalg.norm(coords[:, None, :] - coords[None, :, :], axis=2)\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    # Apply cutoff: distance <= max_dist_km and > 0\n",
        "    mask = (distance <= max_dist_km) & (distance > 0)\n",
        "    inv_dist = np.where(mask, 1.0 / (distance + epsilon), 0.0)\n",
        "    # Set diagonal to zero\n",
        "    np.fill_diagonal(inv_dist, 0)\n",
        "\n",
        "    # Row normalization\n",
        "    row_sums = inv_dist.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1\n",
        "    norm_dist = inv_dist / row_sums\n",
        "    return norm_dist\n",
        "\n",
        "###############################################################################\n",
        "# Read/Construct PCI-based (w), SCI-based (w1), Distance-based (w2, with cutoff), Queen-based (w3)\n",
        "###############################################################################\n",
        "\n",
        "### 1) PCI-based w\n",
        "pci_file_path = \"US_County_PCI_2019.csv\"  # Change to your path\n",
        "pci_df = pd.read_csv(pci_file_path)\n",
        "pci_df['place_i'] = pci_df['place_i'].astype(str).str.lstrip('0')\n",
        "pci_df['place_j'] = pci_df['place_j'].astype(str).str.lstrip('0')\n",
        "filtered_pci_df = pci_df[\n",
        "    pci_df['place_i'].isin(hiv_df['FIPS']) &\n",
        "    pci_df['place_j'].isin(hiv_df['FIPS'])\n",
        "].copy()\n",
        "adj_matrix_pci = filtered_pci_df.pivot_table(\n",
        "    index='place_i', columns='place_j', values='pci', fill_value=0\n",
        ")\n",
        "np.fill_diagonal(adj_matrix_pci.values, 0)\n",
        "common_ids_pci = set(hiv_df['FIPS']).intersection(adj_matrix_pci.index)\n",
        "adj_matrix_pci = adj_matrix_pci.reindex(index=common_ids_pci, columns=common_ids_pci, fill_value=0)\n",
        "mat_val_pci = adj_matrix_pci.values.astype(float)\n",
        "row_sum_pci = mat_val_pci.sum(axis=1, keepdims=True)\n",
        "row_sum_pci[row_sum_pci == 0] = 1\n",
        "mat_norm_pci = mat_val_pci / row_sum_pci\n",
        "fips_list_pci = list(adj_matrix_pci.index)\n",
        "w = libpysal.weights.util.full2W(mat_norm_pci, ids=fips_list_pci)\n",
        "print(\"PCI-based w.n =\", w.n)\n",
        "\n",
        "### 2) SCI-based w1\n",
        "sci_file_path = \"processed_sci_summary.csv\"\n",
        "sci_df = pd.read_csv(sci_file_path)\n",
        "sci_df['user_loc'] = sci_df['user_loc'].astype(str).str.lstrip('0')\n",
        "sci_df['tfr_loc'] = sci_df['tfr_loc'].astype(str).str.lstrip('0')\n",
        "adj_matrix_sci = sci_df.pivot_table(\n",
        "    index='user_loc', columns='tfr_loc', values='tscaled_sci', fill_value=0\n",
        ")\n",
        "# Symmetrize the matrix\n",
        "sci_np = adj_matrix_sci.values\n",
        "sci_np = sci_np + sci_np.T - np.diag(sci_np.diagonal())\n",
        "adj_matrix_sci = pd.DataFrame(sci_np, index=adj_matrix_sci.index, columns=adj_matrix_sci.columns)\n",
        "\n",
        "np.fill_diagonal(adj_matrix_sci.values, 0)\n",
        "common_ids_sci = set(hiv_df['FIPS']).intersection(adj_matrix_sci.index)\n",
        "adj_matrix_sci = adj_matrix_sci.reindex(index=common_ids_sci, columns=common_ids_sci, fill_value=0)\n",
        "mat_val_sci = adj_matrix_sci.values.astype(float)\n",
        "row_sum_sci = mat_val_sci.sum(axis=1, keepdims=True)\n",
        "row_sum_sci[row_sum_sci == 0] = 1\n",
        "mat_norm_sci = mat_val_sci / row_sum_sci\n",
        "fips_list_sci = list(adj_matrix_sci.index)\n",
        "w1 = libpysal.weights.util.full2W(mat_norm_sci, ids=fips_list_sci)\n",
        "print(\"SCI-based w1.n =\", w1.n)\n",
        "\n",
        "### 3) Inverse distance based w2 (with cutoff)\n",
        "coords = hiv_df[['X', 'Y']].values\n",
        "norm_dist_mat = build_distance_matrix(coords, max_dist_km=200)  # Adjust parameter if needed\n",
        "fips_list_dist = hiv_df['FIPS'].tolist()\n",
        "w2 = libpysal.weights.util.full2W(norm_dist_mat, ids=fips_list_dist)\n",
        "print(\"Distance-based w2.n =\", w2.n)\n",
        "\n",
        "### 4) Queen-based w3\n",
        "hiv_df_proj = hiv_df.to_crs(epsg=3857)\n",
        "w3 = Queen.from_dataframe(hiv_df_proj, ids=hiv_df_proj['FIPS'])\n",
        "w3.transform = 'R'\n",
        "print(\"Queen-based w3.n =\", w3.n)\n",
        "\n",
        "### Harmonize all IDs\n",
        "common_all = set(w.id_order) & set(w1.id_order) & set(w2.id_order) & set(w3.id_order) & set(hiv_df['FIPS'])\n",
        "print(\"Common IDs size:\", len(common_all))\n",
        "hiv_df = hiv_df[hiv_df['FIPS'].isin(common_all)].copy()\n",
        "print(\"Final data size after intersection:\", len(hiv_df))\n",
        "w  = subset_W(w, common_all)\n",
        "w1 = subset_W(w1, common_all)\n",
        "w2 = subset_W(w2, common_all)\n",
        "w3 = subset_W(w3, common_all)\n",
        "print(\"Final w.n =\", w.n)\n",
        "print(\"Final w1.n =\", w1.n)\n",
        "print(\"Final w2.n =\", w2.n)\n",
        "print(\"Final w3.n =\", w3.n)\n",
        "N = len(hiv_df)\n",
        "print(\"N =\", N)\n",
        "\n",
        "###############################################################################\n",
        "# C. Construct y and X, and drop rows with NaNs\n",
        "###############################################################################\n",
        "df_temp = hiv_df[['FIPS', 'GonorrheaR', 'ChlamydiaR', 'PercenHIVp',\n",
        "                  'Population', 'UrbanRural', 'Female', 'Old', 'Black',\n",
        "                  'Noinsuranc', 'Poverty', 'crime16to1', 'Dissimilar',\n",
        "                  'X', 'Y', 'geometry']].copy()\n",
        "\n",
        "print(\"NaN sum before dropna:\\n\", df_temp.isna().sum())\n",
        "df_temp = df_temp.dropna(axis=0)\n",
        "print(\"After dropna, row count:\", df_temp.shape[0])\n",
        "\n",
        "valid_ids = set(df_temp['FIPS'])\n",
        "y = df_temp['PercenHIVp'].values  # Dependent variable (example: PercenHIVp)\n",
        "X_cols = ['Population', 'UrbanRural', 'Female', 'Old', 'Black',\n",
        "          'Noinsuranc', 'Poverty', 'crime16to1', 'Dissimilar']\n",
        "X_raw = df_temp[X_cols].values\n",
        "N = X_raw.shape[0]\n",
        "X = np.hstack([np.ones((N, 1)), X_raw])\n",
        "print(\"After dropna, final ID count:\", len(valid_ids))\n",
        "print(\"y.shape =\", y.shape, \"X.shape =\", X.shape)\n",
        "\n",
        "# Subset the 4 weight objects using valid IDs\n",
        "w  = subset_W(w, valid_ids)\n",
        "w1 = subset_W(w1, valid_ids)\n",
        "w2 = subset_W(w2, valid_ids)\n",
        "w3 = subset_W(w3, valid_ids)\n",
        "print(\"After dropna, filtered weights sizes:\")\n",
        "print(\"w.n =\", w.n, \"w1.n =\", w1.n, \"w2.n =\", w2.n, \"w3.n =\", w3.n)\n",
        "\n",
        "###############################################################################\n",
        "# D. Define OLS and multi-weight SEM (with approx Hessian)\n",
        "###############################################################################\n",
        "def fit_ols(y, X):\n",
        "    N, K = X.shape\n",
        "    beta_ols, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    y_hat = X @ beta_ols\n",
        "    residuals = y - y_hat\n",
        "    sse = residuals @ residuals\n",
        "    sigma2 = sse / (N - K)\n",
        "    logL = -(N / 2) * np.log(2 * np.pi * sigma2) - (sse / (2 * sigma2))\n",
        "    AIC = -2 * logL + 2 * K\n",
        "    AICc = AIC + (2 * K * (K + 1)) / (N - K - 1) if (N - K - 1) > 0 else np.nan\n",
        "    return {\n",
        "        'model': 'OLS',\n",
        "        'beta': beta_ols,\n",
        "        'logL': logL,\n",
        "        'AIC': AIC,\n",
        "        'AICc': AICc,\n",
        "        'residuals': residuals\n",
        "    }\n",
        "\n",
        "def neg_loglike_sem(params, y, X, W_list):\n",
        "    N, K = X.shape\n",
        "    m = len(W_list)\n",
        "    lam = params[:m]\n",
        "    beta = params[m:]\n",
        "\n",
        "    I = np.eye(N)\n",
        "    A = I.copy()\n",
        "    for i in range(m):\n",
        "        A -= lam[i] * W_list[i]\n",
        "\n",
        "    sign, logdetA = np.linalg.slogdet(A)\n",
        "    if sign <= 0:\n",
        "        return 1e12\n",
        "\n",
        "    e = y - X @ beta\n",
        "    v = A @ e\n",
        "    dof = N - (K + m)\n",
        "    if dof <= 0:\n",
        "        return 1e12\n",
        "    sse = v @ v\n",
        "    sigma2 = sse / dof\n",
        "    if sigma2 <= 0:\n",
        "        return 1e12\n",
        "\n",
        "    logL = logdetA - (N / 2) * np.log(2 * np.pi * sigma2) - (sse / (2 * sigma2))\n",
        "    return -logL\n",
        "\n",
        "def fit_sem_multi(y, X, W_list):\n",
        "    N, K = X.shape\n",
        "    m = len(W_list)\n",
        "    # Initial guess using OLS estimates\n",
        "    beta_ols, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    init_params = np.concatenate([np.zeros(m), beta_ols])\n",
        "    bounds = [(-0.99, 0.99)] * m + [(-np.inf, np.inf)] * K\n",
        "\n",
        "    res = minimize(\n",
        "        neg_loglike_sem,\n",
        "        init_params,\n",
        "        args=(y, X, W_list),\n",
        "        method='L-BFGS-B',\n",
        "        bounds=bounds\n",
        "    )\n",
        "    param_hat = res.x\n",
        "    lam_hat = param_hat[:m]\n",
        "    beta_hat = param_hat[m:]\n",
        "    neg_ll = res.fun\n",
        "    logL = -neg_ll\n",
        "    p_total = m + K\n",
        "\n",
        "    AIC = -2 * logL + 2 * p_total\n",
        "    AICc = AIC + (2 * p_total * (p_total + 1)) / (N - p_total - 1) if (N - p_total - 1) > 0 else np.nan\n",
        "\n",
        "    # Residuals calculation\n",
        "    I = np.eye(N)\n",
        "    A = I.copy()\n",
        "    for i in range(m):\n",
        "        A -= lam_hat[i] * W_list[i]\n",
        "    e = y - X @ beta_hat\n",
        "    v = A @ e\n",
        "\n",
        "    # Hessian approximation for standard errors\n",
        "    if hasattr(res, 'hess_inv'):\n",
        "        hess_inv_approx = res.hess_inv\n",
        "        if hasattr(hess_inv_approx, 'todense'):\n",
        "            hess_inv_approx = hess_inv_approx.todense()\n",
        "        if hess_inv_approx.shape == (p_total, p_total):\n",
        "            se = np.sqrt(np.diag(hess_inv_approx))\n",
        "            t_stats = param_hat / se\n",
        "            p_vals = 2 * (1 - norm.cdf(np.abs(t_stats)))\n",
        "        else:\n",
        "            se = np.full(p_total, np.nan)\n",
        "            t_stats = np.full(p_total, np.nan)\n",
        "            p_vals = np.full(p_total, np.nan)\n",
        "    else:\n",
        "        se = np.full(p_total, np.nan)\n",
        "        t_stats = np.full(p_total, np.nan)\n",
        "        p_vals = np.full(p_total, np.nan)\n",
        "\n",
        "    return {\n",
        "        'model': f'SEM_{m}_weights',\n",
        "        'lam': lam_hat,\n",
        "        'beta': beta_hat,\n",
        "        'logL': logL,\n",
        "        'AIC': AIC,\n",
        "        'AICc': AICc,\n",
        "        'residuals': v,\n",
        "        'converged': res.success,\n",
        "        'message': res.message,\n",
        "        'se_hessian': se,\n",
        "        't_hessian': t_stats,\n",
        "        'p_hessian': p_vals\n",
        "    }\n",
        "\n",
        "###############################################################################\n",
        "# E. Flatten matrices & check multicollinearity\n",
        "###############################################################################\n",
        "def flatten_W(W_mat):\n",
        "    \"\"\"\n",
        "    Flatten an NxN matrix (excluding the diagonal) into a vector.\n",
        "    \"\"\"\n",
        "    N = W_mat.shape[0]\n",
        "    mask = ~np.eye(N, dtype=bool)\n",
        "    return W_mat[mask]\n",
        "\n",
        "def check_correlation_among(W_list, names=None):\n",
        "    \"\"\"\n",
        "    Flatten all matrices in W_list and combine into one DataFrame,\n",
        "    then output the correlation matrix and VIF.\n",
        "    Skip VIF if only one column is present.\n",
        "    \"\"\"\n",
        "    if names is None:\n",
        "        names = [f\"W{i}\" for i in range(len(W_list))]\n",
        "\n",
        "    data_dict = {}\n",
        "    for i, wmat in enumerate(W_list):\n",
        "        vec = flatten_W(wmat)\n",
        "        data_dict[names[i]] = vec\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    print(\"\\n------ Checking correlation among these matrices ------\")\n",
        "    # If only one column, cannot calculate VIF\n",
        "    if df.shape[1] < 2:\n",
        "        single_col = df.columns[0]\n",
        "        print(f\"(Only 1 column: '{single_col}'. No correlation or VIF computed for single column scenario.)\")\n",
        "        return\n",
        "    else:\n",
        "        corr = df.corr()\n",
        "        print(\"Correlation matrix:\")\n",
        "        print(corr)\n",
        "\n",
        "        print(\"\\nVIF:\")\n",
        "        X_vif = df.values\n",
        "        for i, col in enumerate(df.columns):\n",
        "            vif_val = variance_inflation_factor(X_vif, i)\n",
        "            print(f\"{col}: {vif_val:.3f}\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "\n",
        "###############################################################################\n",
        "# F. Stepwise model construction: single and pairwise combinations\n",
        "###############################################################################\n",
        "arr_w, _ = w.full()\n",
        "arr_w1, _ = w1.full()\n",
        "arr_w2, _ = w2.full()\n",
        "arr_w3, _ = w3.full()\n",
        "\n",
        "all_matrices = {\n",
        "    \"WP\": arr_w,   # PCI-based\n",
        "    \"WS\": arr_w1,  # SCI-based\n",
        "    \"WD\": arr_w2,  # Distance-based\n",
        "    \"WQ\": arr_w3   # Queen-based\n",
        "}\n",
        "\n",
        "# 1) Single matrix models\n",
        "from itertools import combinations\n",
        "\n",
        "single_tasks = []\n",
        "for k in all_matrices:\n",
        "    single_tasks.append((f\"SEM_1_{k}\", [all_matrices[k]]))\n",
        "\n",
        "# 2) Two-matrix combinations\n",
        "pair_tasks = []\n",
        "keys_list = list(all_matrices.keys())\n",
        "for combo in combinations(keys_list, 2):\n",
        "    name_combo = \"_\".join(combo)\n",
        "    arr_list = [all_matrices[c] for c in combo]\n",
        "    pair_tasks.append((f\"SEM_2_{name_combo}\", arr_list))\n",
        "\n",
        "task_list = single_tasks + pair_tasks\n",
        "\n",
        "###############################################################################\n",
        "# G. Execute SEM fitting and output results\n",
        "###############################################################################\n",
        "models = {}\n",
        "print(\"\\n================ Starting stepwise model testing =================\\n\")\n",
        "\n",
        "for (model_name, W_list) in task_list:\n",
        "    # First: check multicollinearity among the weight matrices\n",
        "    matrix_names = [f\"{model_name}_mat{i}\" for i in range(len(W_list))]\n",
        "    check_correlation_among(W_list, names=matrix_names)\n",
        "\n",
        "    # Fit the SEM using the provided weight matrices\n",
        "    resdict = fit_sem_multi(y, X, W_list)\n",
        "    models[model_name] = resdict\n",
        "\n",
        "# Finally, add OLS as a benchmark model\n",
        "ols_res = fit_ols(y, X)\n",
        "models[\"OLS\"] = ols_res\n",
        "\n",
        "print(\"\\n=== Model: OLS ===\")\n",
        "print(f\"logL = {ols_res['logL']:.3f}, AIC = {ols_res['AIC']:.3f}, AICc = {ols_res['AICc']:.3f}\")\n",
        "\n",
        "for name, res in models.items():\n",
        "    if name == \"OLS\":\n",
        "        continue\n",
        "    print(f\"\\n=== Model: {name} ===\")\n",
        "    print(f\"logL = {res['logL']:.3f}, AIC = {res['AIC']:.3f}, AICc = {res['AICc']:.3f}\")\n",
        "    print(\"Converged?\", res['converged'], \"| Message:\", res['message'])\n",
        "    lam_ = res['lam']\n",
        "    beta_ = res['beta']\n",
        "    se_all = res['se_hessian']\n",
        "    t_all = res['t_hessian']\n",
        "    p_all = res['p_hessian']\n",
        "    m_ = len(lam_)\n",
        "\n",
        "    print(\"Spatial parameters (lambda):\")\n",
        "    for i in range(m_):\n",
        "        print(f\"  lam[{i+1}] = {lam_[i]:.6f}, SE = {se_all[i]:.6f}, t = {t_all[i]:.3f}, p = {p_all[i]:.4f}\")\n",
        "\n",
        "    print(\"Regression coefficients (beta):\")\n",
        "    for j in range(len(beta_)):\n",
        "        idx = m_ + j\n",
        "        print(f\"  beta[{j}] = {beta_[j]:.6f}, SE = {se_all[idx]:.6f}, t = {t_all[idx]:.3f}, p = {p_all[idx]:.4f}\")\n",
        "\n",
        "print(\"\\n================ Completed: Stepwise model testing, multicollinearity check & distance cutoff =================\\n\")\n"
      ],
      "metadata": {
        "id": "WmgA4m6ANCKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SLM"
      ],
      "metadata": {
        "id": "ptQgj_NEP_P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "\n",
        "import libpysal\n",
        "from libpysal.weights import util, W, Queen\n",
        "from scipy.optimize import minimize\n",
        "from numpy.linalg import slogdet\n",
        "from scipy.stats import norm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Use joblib for parallel processing if needed\n",
        "from joblib import Parallel, delayed\n",
        "from itertools import combinations\n",
        "\n",
        "###############################################################################\n",
        "# A. Read Shapefile and Required Fields, Ensuring Unique FIPS Codes\n",
        "###############################################################################\n",
        "shp_path = \"US_HIV_Merged_total.shp\"  # Change to your own path\n",
        "gdf = gpd.read_file(shp_path)\n",
        "\n",
        "if 'FIPS' not in gdf.columns:\n",
        "    raise KeyError(\"Shapefile is missing the 'FIPS' column\")\n",
        "gdf['FIPS'] = gdf['FIPS'].astype(str).str.lstrip('0')\n",
        "gdf = gdf.drop_duplicates(subset=['FIPS'])\n",
        "\n",
        "# Extract required fields (adjust as needed)\n",
        "hiv_df = gdf[['FIPS', 'GonorrheaR', 'ChlamydiaR', 'PercenHIVp',\n",
        "              'Population', 'UrbanRural', 'Female', 'Old', 'Black', 'Noinsuranc',\n",
        "              'Poverty', 'crime16to1', 'Dissimilar', 'X', 'Y', 'geometry']].copy()\n",
        "print(\"Loaded data (first 5 rows):\")\n",
        "print(hiv_df.head())\n",
        "\n",
        "###############################################################################\n",
        "# B. Helper Functions: subset_W, build_distance_matrix\n",
        "###############################################################################\n",
        "def subset_W(w, subset_ids):\n",
        "    \"\"\"\n",
        "    From the given W object, keep only the rows and columns corresponding\n",
        "    to the IDs in subset_ids, and return a new W object.\n",
        "    \"\"\"\n",
        "    from libpysal.weights import W\n",
        "    subset_ids = set(subset_ids)\n",
        "    new_neighbors = {}\n",
        "    new_weights = {}\n",
        "    for i in w.id_order:\n",
        "        if i not in subset_ids:\n",
        "            continue\n",
        "        old_neighbors = w.neighbors[i]\n",
        "        old_weights = w.weights[i]\n",
        "        filtered_neighbors = []\n",
        "        filtered_weights = []\n",
        "        for nb, wt in zip(old_neighbors, old_weights):\n",
        "            if nb in subset_ids:\n",
        "                filtered_neighbors.append(nb)\n",
        "                filtered_weights.append(wt)\n",
        "        new_neighbors[i] = filtered_neighbors\n",
        "        new_weights[i] = filtered_weights\n",
        "    return W(new_neighbors, new_weights, ids=list(subset_ids))\n",
        "\n",
        "def build_distance_matrix(coords, max_dist_km=200):\n",
        "    \"\"\"\n",
        "    Generate an inverse-distance matrix with a cutoff and perform row normalization.\n",
        "    Distances greater than max_dist_km are set to 0, and the diagonal is set to 0.\n",
        "    Returns an (N x N) numpy array.\n",
        "    \"\"\"\n",
        "    distance = np.linalg.norm(coords[:, None, :] - coords[None, :, :], axis=2)\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    # Apply cutoff: only distances <= max_dist_km and > 0\n",
        "    mask = (distance <= max_dist_km) & (distance > 0)\n",
        "    inv_dist = np.where(mask, 1.0/(distance + epsilon), 0.0)\n",
        "    np.fill_diagonal(inv_dist, 0)\n",
        "\n",
        "    # Row normalization\n",
        "    row_sums = inv_dist.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1\n",
        "    norm_dist = inv_dist / row_sums\n",
        "    return norm_dist\n",
        "\n",
        "###############################################################################\n",
        "# C. Construct the 4 Spatial Weights (PCI, SCI, Distance, Queen)\n",
        "###############################################################################\n",
        "# 1) PCI-based Weight (w)\n",
        "pci_file_path = \"US_County_PCI_2019.csv\"  # Change your path\n",
        "pci_df = pd.read_csv(pci_file_path)\n",
        "pci_df['place_i'] = pci_df['place_i'].astype(str).str.lstrip('0')\n",
        "pci_df['place_j'] = pci_df['place_j'].astype(str).str.lstrip('0')\n",
        "filtered_pci_df = pci_df[\n",
        "    pci_df['place_i'].isin(hiv_df['FIPS']) & pci_df['place_j'].isin(hiv_df['FIPS'])\n",
        "].copy()\n",
        "adj_matrix_pci = filtered_pci_df.pivot_table(\n",
        "    index='place_i', columns='place_j', values='pci', fill_value=0\n",
        ")\n",
        "np.fill_diagonal(adj_matrix_pci.values, 0)\n",
        "common_ids_pci = set(hiv_df['FIPS']).intersection(adj_matrix_pci.index)\n",
        "adj_matrix_pci = adj_matrix_pci.reindex(index=common_ids_pci, columns=common_ids_pci, fill_value=0)\n",
        "mat_val_pci = adj_matrix_pci.values.astype(float)\n",
        "row_sum_pci = mat_val_pci.sum(axis=1, keepdims=True)\n",
        "row_sum_pci[row_sum_pci==0] = 1\n",
        "mat_norm_pci = mat_val_pci / row_sum_pci\n",
        "fips_list_pci = list(adj_matrix_pci.index)\n",
        "w = libpysal.weights.util.full2W(mat_norm_pci, ids=fips_list_pci)\n",
        "print(\"PCI-based w.n =\", w.n)\n",
        "\n",
        "# 2) SCI-based Weight (w1)\n",
        "sci_file_path = \"processed_sci_summary.csv\"  # Change your path\n",
        "sci_df = pd.read_csv(sci_file_path)\n",
        "sci_df['user_loc'] = sci_df['user_loc'].astype(str).str.lstrip('0')\n",
        "sci_df['tfr_loc'] = sci_df['tfr_loc'].astype(str).str.lstrip('0')\n",
        "adj_matrix_sci = sci_df.pivot_table(\n",
        "    index='user_loc', columns='tfr_loc', values='tscaled_sci', fill_value=0\n",
        ")\n",
        "# Symmetrize the matrix\n",
        "sci_np = adj_matrix_sci.values\n",
        "sci_np = sci_np + sci_np.T - np.diag(sci_np.diagonal())\n",
        "adj_matrix_sci = pd.DataFrame(sci_np, index=adj_matrix_sci.index, columns=adj_matrix_sci.columns)\n",
        "np.fill_diagonal(adj_matrix_sci.values, 0)\n",
        "common_ids_sci = set(hiv_df['FIPS']).intersection(adj_matrix_sci.index)\n",
        "adj_matrix_sci = adj_matrix_sci.reindex(index=common_ids_sci, columns=common_ids_sci, fill_value=0)\n",
        "mat_val_sci = adj_matrix_sci.values.astype(float)\n",
        "row_sum_sci = mat_val_sci.sum(axis=1, keepdims=True)\n",
        "row_sum_sci[row_sum_sci==0] = 1\n",
        "mat_norm_sci = mat_val_sci / row_sum_sci\n",
        "fips_list_sci = list(adj_matrix_sci.index)\n",
        "w1 = libpysal.weights.util.full2W(mat_norm_sci, ids=fips_list_sci)\n",
        "print(\"SCI-based w1.n =\", w1.n)\n",
        "\n",
        "# 3) Distance-based Weight (w2) with a 200km cutoff\n",
        "coords = hiv_df[['X', 'Y']].values\n",
        "norm_dist_mat = build_distance_matrix(coords, max_dist_km=200)\n",
        "fips_list_dist = hiv_df['FIPS'].tolist()\n",
        "w2 = libpysal.weights.util.full2W(norm_dist_mat, ids=fips_list_dist)\n",
        "print(\"Distance-based w2.n =\", w2.n)\n",
        "\n",
        "# 4) Queen-based Weight (w3)\n",
        "hiv_df_proj = hiv_df.to_crs(epsg=3857)\n",
        "w3 = Queen.from_dataframe(hiv_df_proj, ids=hiv_df_proj['FIPS'])\n",
        "w3.transform = 'R'\n",
        "print(\"Queen-based w3.n =\", w3.n)\n",
        "\n",
        "# Harmonize IDs across all weight matrices\n",
        "common_all = set(w.id_order) & set(w1.id_order) & set(w2.id_order) & set(w3.id_order) & set(hiv_df['FIPS'])\n",
        "print(\"Common IDs size:\", len(common_all))\n",
        "hiv_df = hiv_df[hiv_df['FIPS'].isin(common_all)].copy()\n",
        "print(\"Final data size after intersection:\", len(hiv_df))\n",
        "w = subset_W(w, common_all)\n",
        "w1 = subset_W(w1, common_all)\n",
        "w2 = subset_W(w2, common_all)\n",
        "w3 = subset_W(w3, common_all)\n",
        "print(\"Final w.n =\", w.n, \"w1.n =\", w1.n, \"w2.n =\", w2.n, \"w3.n =\", w3.n)\n",
        "N = len(hiv_df)\n",
        "print(\"N =\", N)\n",
        "\n",
        "###############################################################################\n",
        "# D. Construct y and X, and drop rows with NaN values\n",
        "###############################################################################\n",
        "df_temp = hiv_df[['FIPS', 'GonorrheaR', 'ChlamydiaR', 'PercenHIVp',\n",
        "                  'Population', 'UrbanRural', 'Female', 'Old', 'Black',\n",
        "                  'Noinsuranc', 'Poverty', 'crime16to1', 'Dissimilar',\n",
        "                  'X', 'Y', 'geometry']].copy()\n",
        "\n",
        "print(\"NaN sum before dropna:\\n\", df_temp.isna().sum())\n",
        "df_temp = df_temp.dropna(axis=0)\n",
        "print(\"After dropna, row count:\", df_temp.shape[0])\n",
        "\n",
        "valid_ids = set(df_temp['FIPS'])\n",
        "y = df_temp['PercenHIVp'].values  # Dependent variable\n",
        "X_cols = ['Population', 'UrbanRural', 'Female', 'Old', 'Black',\n",
        "          'Noinsuranc', 'Poverty', 'crime16to1', 'Dissimilar']\n",
        "X_raw = df_temp[X_cols].values\n",
        "N = X_raw.shape[0]\n",
        "X = np.hstack([np.ones((N, 1)), X_raw])\n",
        "print(\"After dropna, final ID count:\", len(valid_ids))\n",
        "print(\"y.shape =\", y.shape, \"X.shape =\", X.shape)\n",
        "\n",
        "# Subset the 4 spatial weights using valid IDs\n",
        "w = subset_W(w, valid_ids)\n",
        "w1 = subset_W(w1, valid_ids)\n",
        "w2 = subset_W(w2, valid_ids)\n",
        "w3 = subset_W(w3, valid_ids)\n",
        "print(\"After dropna, filtered weights sizes:\")\n",
        "print(\"w.n =\", w.n, \"w1.n =\", w1.n, \"w2.n =\", w2.n, \"w3.n =\", w3.n)\n",
        "\n",
        "###############################################################################\n",
        "# E. Define OLS and Multi-Weight SLM (Spatial Lag Model) MLE\n",
        "###############################################################################\n",
        "def fit_ols(y, X):\n",
        "    N, K = X.shape\n",
        "    beta_ols, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    y_hat = X @ beta_ols\n",
        "    residuals = y - y_hat\n",
        "    sse = residuals @ residuals\n",
        "    sigma2 = sse / (N - K)\n",
        "    logL = -(N / 2) * np.log(2 * np.pi * sigma2) - (sse / (2 * sigma2))\n",
        "    AIC = -2 * logL + 2 * K\n",
        "    AICc = AIC + (2 * K * (K + 1)) / (N - K - 1) if (N - K - 1) > 0 else np.nan\n",
        "    return {\n",
        "        'model': 'OLS',\n",
        "        'beta': beta_ols,\n",
        "        'logL': logL,\n",
        "        'AIC': AIC,\n",
        "        'AICc': AICc,\n",
        "        'residuals': residuals\n",
        "    }\n",
        "\n",
        "def neg_loglike_slm(params, y, X, W_list):\n",
        "    \"\"\"\n",
        "    Negative log-likelihood for the Spatial Lag Model (SLM):\n",
        "       y = Σ_i (rho_i * W_i) y + X β + e\n",
        "       => A = I - Σ_i (rho_i * W_i)\n",
        "       => e = A y - X β\n",
        "       => SSE = e'e, sigma2 = SSE / dof\n",
        "       => log-likelihood = log|A| - (N/2)*ln(2πsigma2) - SSE/(2sigma2)\n",
        "    \"\"\"\n",
        "    N, K = X.shape\n",
        "    m = len(W_list)\n",
        "    rho = params[:m]\n",
        "    beta = params[m:]\n",
        "\n",
        "    I = np.eye(N)\n",
        "    A = I.copy()\n",
        "    for i in range(m):\n",
        "        A -= rho[i] * W_list[i]\n",
        "\n",
        "    sign, logdetA = np.linalg.slogdet(A)\n",
        "    if sign <= 0:\n",
        "        return 1e12\n",
        "\n",
        "    # Compute residuals: e = A y - X β\n",
        "    e = A @ y - X @ beta\n",
        "    dof = N - (K + m)\n",
        "    if dof <= 0:\n",
        "        return 1e12\n",
        "    sse = e @ e\n",
        "    sigma2 = sse / dof\n",
        "    if sigma2 <= 0:\n",
        "        return 1e12\n",
        "\n",
        "    logL = logdetA - (N / 2) * np.log(2 * np.pi * sigma2) - (sse / (2 * sigma2))\n",
        "    return -logL\n",
        "\n",
        "def fit_slm_multi(y, X, W_list):\n",
        "    \"\"\"\n",
        "    Multi-weight SLM MLE: y = Σ_i (rho_i * W_i) y + X β + e\n",
        "    \"\"\"\n",
        "    N, K = X.shape\n",
        "    m = len(W_list)\n",
        "    # Initial guess: rho=0, beta = OLS estimates\n",
        "    beta_ols, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
        "    init_params = np.concatenate([np.zeros(m), beta_ols])\n",
        "    bounds = [(-0.99, 0.99)] * m + [(-np.inf, np.inf)] * K\n",
        "\n",
        "    res = minimize(\n",
        "        neg_loglike_slm,\n",
        "        init_params,\n",
        "        args=(y, X, W_list),\n",
        "        method='L-BFGS-B',\n",
        "        bounds=bounds\n",
        "    )\n",
        "    param_hat = res.x\n",
        "    rho_hat = param_hat[:m]\n",
        "    beta_hat = param_hat[m:]\n",
        "    neg_ll = res.fun\n",
        "    logL = -neg_ll\n",
        "    p_total = m + K\n",
        "\n",
        "    AIC = -2 * logL + 2 * p_total\n",
        "    AICc = AIC + (2 * p_total * (p_total + 1)) / (N - p_total - 1) if (N - p_total - 1) > 0 else np.nan\n",
        "\n",
        "    # Residuals: e = A y - X β\n",
        "    I = np.eye(N)\n",
        "    A = I.copy()\n",
        "    for i in range(m):\n",
        "        A -= rho_hat[i] * W_list[i]\n",
        "    e = A @ y - X @ beta_hat\n",
        "\n",
        "    # Approximate Hessian for standard errors\n",
        "    if hasattr(res, 'hess_inv'):\n",
        "        hess_inv_approx = res.hess_inv\n",
        "        if hasattr(hess_inv_approx, 'todense'):\n",
        "            hess_inv_approx = hess_inv_approx.todense()\n",
        "        if hess_inv_approx.shape == (p_total, p_total):\n",
        "            se = np.sqrt(np.diag(hess_inv_approx))\n",
        "            t_stats = param_hat / se\n",
        "            p_vals = 2 * (1 - norm.cdf(np.abs(t_stats)))\n",
        "        else:\n",
        "            se = np.full(p_total, np.nan)\n",
        "            t_stats = np.full(p_total, np.nan)\n",
        "            p_vals = np.full(p_total, np.nan)\n",
        "    else:\n",
        "        se = np.full(p_total, np.nan)\n",
        "        t_stats = np.full(p_total, np.nan)\n",
        "        p_vals = np.full(p_total, np.nan)\n",
        "\n",
        "    return {\n",
        "        'model': f'SLM_{m}_weights',\n",
        "        'rho': rho_hat,\n",
        "        'beta': beta_hat,\n",
        "        'logL': logL,\n",
        "        'AIC': AIC,\n",
        "        'AICc': AICc,\n",
        "        'residuals': e,\n",
        "        'converged': res.success,\n",
        "        'message': res.message,\n",
        "        'se_hessian': se,\n",
        "        't_hessian': t_stats,\n",
        "        'p_hessian': p_vals\n",
        "    }\n",
        "\n",
        "###############################################################################\n",
        "# F. Multicollinearity Check Functions\n",
        "###############################################################################\n",
        "def flatten_W(W_mat):\n",
        "    \"\"\"\n",
        "    Flatten an NxN matrix (excluding the diagonal) into a vector.\n",
        "    \"\"\"\n",
        "    N = W_mat.shape[0]\n",
        "    mask = ~np.eye(N, dtype=bool)\n",
        "    return W_mat[mask]\n",
        "\n",
        "def check_correlation_among(W_list, names=None):\n",
        "    if names is None:\n",
        "        names = [f\"W{i}\" for i in range(len(W_list))]\n",
        "    data_dict = {}\n",
        "    for i, wmat in enumerate(W_list):\n",
        "        vec = flatten_W(wmat)\n",
        "        data_dict[names[i]] = vec\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    print(\"\\n------ Checking correlation among these matrices ------\")\n",
        "    if df.shape[1] < 2:\n",
        "        single_col = df.columns[0]\n",
        "        print(f\"(Only 1 column: '{single_col}'. No correlation or VIF for single column scenario.)\")\n",
        "        return\n",
        "    else:\n",
        "        corr = df.corr()\n",
        "        print(\"Correlation matrix:\")\n",
        "        print(corr)\n",
        "\n",
        "        print(\"\\nVIF:\")\n",
        "        X_vif = df.values\n",
        "        for i, col in enumerate(df.columns):\n",
        "            vif_val = variance_inflation_factor(X_vif, i)\n",
        "            print(f\"{col}: {vif_val:.3f}\")\n",
        "    print(\"-------------------------------------------------------\")\n",
        "\n",
        "###############################################################################\n",
        "# G. Generate Single and Pairwise Combinations, Fit SLM, and Output Results\n",
        "###############################################################################\n",
        "arr_w, _ = w.full()\n",
        "arr_w1, _ = w1.full()\n",
        "arr_w2, _ = w2.full()\n",
        "arr_w3, _ = w3.full()\n",
        "\n",
        "all_matrices = {\n",
        "    \"WP\": arr_w,  # PCI\n",
        "    \"WS\": arr_w1, # SCI\n",
        "    \"WD\": arr_w2, # Distance\n",
        "    \"WQ\": arr_w3  # Queen\n",
        "}\n",
        "\n",
        "# 1) Single-matrix models\n",
        "single_tasks = []\n",
        "for k in all_matrices:\n",
        "    single_tasks.append((f\"SLM_1_{k}\", [all_matrices[k]]))\n",
        "\n",
        "# 2) Two-matrix combinations\n",
        "pair_tasks = []\n",
        "keys_list = list(all_matrices.keys())\n",
        "for combo in combinations(keys_list, 2):\n",
        "    name_combo = \"_\".join(combo)\n",
        "    arr_list = [all_matrices[c] for c in combo]\n",
        "    pair_tasks.append((f\"SLM_2_{name_combo}\", arr_list))\n",
        "\n",
        "task_list = single_tasks + pair_tasks\n",
        "\n",
        "models = {}\n",
        "print(\"\\n================ Starting Stepwise SLM Model Testing =================\\n\")\n",
        "\n",
        "# Execute SLM fitting for each task\n",
        "for (model_name, W_list) in task_list:\n",
        "    matrix_names = [f\"{model_name}_mat{i}\" for i in range(len(W_list))]\n",
        "    check_correlation_among(W_list, names=matrix_names)\n",
        "\n",
        "    # Fit the multi-weight SLM\n",
        "    resdict = fit_slm_multi(y, X, W_list)\n",
        "    models[model_name] = resdict\n",
        "\n",
        "# OLS as benchmark\n",
        "ols_res = fit_ols(y, X)\n",
        "models[\"OLS\"] = ols_res\n",
        "\n",
        "print(\"\\n=== Model: OLS ===\")\n",
        "print(f\"logL = {ols_res['logL']:.3f}, AIC = {ols_res['AIC']:.3f}, AICc = {ols_res['AICc']:.3f}\")\n",
        "\n",
        "# Print results for each model\n",
        "for name, res in models.items():\n",
        "    if name == \"OLS\":\n",
        "        continue\n",
        "    print(f\"\\n=== Model: {name} ===\")\n",
        "    print(f\"logL = {res['logL']:.3f}, AIC = {res['AIC']:.3f}, AICc = {res['AICc']:.3f}\")\n",
        "    print(\"Converged?\", res['converged'], \"| Message:\", res['message'])\n",
        "\n",
        "    rho_ = res['rho']\n",
        "    beta_ = res['beta']\n",
        "    se_ = res['se_hessian']\n",
        "    t_ = res['t_hessian']\n",
        "    p_ = res['p_hessian']\n",
        "    m_ = len(rho_)\n",
        "\n",
        "    print(\"Spatial parameters (rho):\")\n",
        "    for i in range(m_):\n",
        "        print(f\"  rho[{i+1}] = {rho_[i]:.6f}, SE = {se_[i]:.6f}, t = {t_[i]:.3f}, p = {p_[i]:.4f}\")\n",
        "\n",
        "    print(\"Regression coefficients (beta):\")\n",
        "    for j in range(len(beta_)):\n",
        "        idx = m_ + j\n",
        "        print(f\"  beta[{j}] = {beta_[j]:.6f}, SE = {se_[idx]:.6f}, t = {t_[idx]:.3f}, p = {p_[idx]:.4f}\")\n",
        "\n",
        "print(\"\\n================ Completed: Multi-weight SLM Model Fitting & Checks =================\\n\")\n"
      ],
      "metadata": {
        "id": "csGfcDvIO6Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Disease types"
      ],
      "metadata": {
        "id": "eWL6w477SWOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For different diseases, simply modify the dependent variable assignment in the code above.\n",
        "\n",
        "If you are modeling HIV prevalence, use:\n",
        "\n",
        "\"y = df_temp['PercenHIVp'].values  # Dependent variable\"\n",
        "\n",
        "If you are modeling Gonorrhea rates, use:\n",
        "\n",
        "\"y = df_temp['GonorrheaR'].values  # Dependent variable\"\n",
        "\n",
        "If you are modeling Chlamydia rates, use:\n",
        "\n",
        "\"y = df_temp['ChlamydiaR'].values  # Dependent variable\""
      ],
      "metadata": {
        "id": "cIVe2pZCQEzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CBSA data"
      ],
      "metadata": {
        "id": "yizOpy8wNEfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "For CBSA data, simply modify the dataset assignment in the code above:\n",
        "\n",
        "shp_path = \"CBSA_HIV_Merged.shp\""
      ],
      "metadata": {
        "id": "VQgSxa2zNVF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DeepSouth Data"
      ],
      "metadata": {
        "id": "44ZcoLmUNNK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "For Deep South data, simply modify the dataset assignment in the code above:\n",
        "\n",
        "shp_path = \"DeepSouth_HIV_Merged.shp\""
      ],
      "metadata": {
        "id": "kYs2Fu4MNVlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}